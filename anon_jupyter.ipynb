{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae552ad1",
   "metadata": {},
   "source": [
    "## Anonmymizing the customer_information.csv and calculating the k-anonymity of the new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545484e4",
   "metadata": {},
   "source": [
    "Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "f8e90ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "from geopy.geocoders import Nominatim\n",
    "import country_converter as coco\n",
    "from cryptography.fernet import Fernet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "8fe288a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Helper functions\n",
    "#\n",
    "\n",
    "northern_countries = [\"Svalbard & Jan Mayen Islands\"]\n",
    "southern_countries = [\"Micronesia\"]\n",
    "\n",
    "# Parse country into more workeable format\n",
    "def parse_country(country_name):\n",
    "    country = coco.convert(country_name, to='name_short', include_obsolete=True)\n",
    "    return country\n",
    "\n",
    "def country_to_hemisphere(country_name):\n",
    "    try:  \n",
    "        if country_name in southern_countries: # Hard-coded fix for unmatched territories\n",
    "            return \"Southern Hemisphere\" \n",
    "        elif country_name in northern_countries:\n",
    "            return \"Northern Hemisphere\"\n",
    "        else:\n",
    "            return (\"Southern\" if Nominatim(user_agent=\"CDM\").geocode(parse_country(country_name)).latitude < 0 else \"Northern\") + \" Hemisphere\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"Error\"\n",
    "    \n",
    "# SHA hash using key and salt\n",
    "def hash(to_hash, key):\n",
    "    salt = os.urandom(16)\n",
    "    h = hashlib.sha256()\n",
    "    h.update(key)\n",
    "    h.update(salt)\n",
    "    h.update(to_hash.encode())\n",
    "    return to_hash, h.hexdigest(), salt.hex()\n",
    "\n",
    "# Encrypt and save as encrypted file; specify file to encrypt, encrypted file destination, and destination key location\n",
    "def encrypt(to_encrypt, file_destination, key_location):\n",
    "    key = Fernet.generate_key() # AES in CBC mode with a 128-bit key for encryption\n",
    "    fernet = Fernet(key)\n",
    "    \n",
    "    with open(key_location, 'wb') as f:\n",
    "        f.write(key)\n",
    "    \n",
    "    with open(to_encrypt, 'rb') as f:\n",
    "        plaintext = f.read()   \n",
    "    \n",
    "    encrypted = fernet.encrypt(plaintext)\n",
    "    with open(file_destination, 'wb') as e:\n",
    "        e.write(encrypted)\n",
    "\n",
    "# Decrypt and save as plaintext file; specify file to decrypt, decrypted file destination, and key location\n",
    "def decrypt(to_decrypt, file_destination, key_location):\n",
    "    with open(key_location, 'rb') as f:\n",
    "        key = f.read()\n",
    "        \n",
    "    fernet = Fernet(key)\n",
    "\n",
    "    with open(to_decrypt, 'rb') as f:\n",
    "        encrypted = f.read()\n",
    "\n",
    "    decrypted = fernet.decrypt(encrypted)\n",
    "    with open(file_destination, 'wb') as f:\n",
    "        f.write(decrypted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3071a7",
   "metadata": {},
   "source": [
    "Loading the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "b5970dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data to be anonymised\n",
    "data = pd.read_csv(\"Data/customer_information.csv\")\n",
    "\n",
    "# Declaring variables\n",
    "postcode_dictionary = pd.read_csv('Data/postcode_region.csv')\n",
    "\n",
    "# Create anon_data variable as initial data with unneeded direct identifiers dropped\n",
    "#anon_data = data.drop(['given_name', 'surname', 'phone_number', 'national_insurance_number', 'bank_account_number'], axis=1)\n",
    "anon_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "8ae51be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean NIN formatting and assign Sample ID as a hashed form of the NIN\n",
    "key = os.urandom(16)\n",
    "data[\"national_insurance_number\"], anon_data['Sample.ID'], salts = zip(*data[\"national_insurance_number\"].apply(\n",
    "    lambda x: hash(re.sub(r'(.{2})(?!$)','\\\\1 ', x.replace(' ', '') ), key)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "da04a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reference table between NIN and respective hashed NIN\n",
    "reference_table = pd.DataFrame()\n",
    "reference_table['Hashed.NIN'] = anon_data['Sample.ID']\n",
    "reference_table['Salt'] = salts\n",
    "reference_table['Key'] = key.hex()\n",
    "reference_table['NIN'] = data['national_insurance_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "06a31d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign gender\n",
    "anon_data['Gender'] = data['gender']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "32d0a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banding birth date\n",
    "birthyears = pd.DatetimeIndex(data['birthdate']).year\n",
    "# Band the birth years into 5-year intervals\n",
    "anon_data['Birthyear'] = pd.cut(birthyears, np.arange(birthyears.min(), birthyears.max()+20, 20), right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "82f9f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping education level, postcode, and country of birth\n",
    "\n",
    "# Assign education level as banded education level\n",
    "anon_data['Education.Level'] = data['education_level'].map(lambda x: \"Higher\" if x in [\"bachelor\", \"masters\", \"phD\"] else \"BasicOther\")\n",
    "\n",
    "# Assign UK country derived from postcode\n",
    "anon_data['Postcode'] = data['postcode'].apply(lambda x: re.search('[a-zA-Z]*', x).group(0))\n",
    "anon_data = pd.merge(anon_data, postcode_dictionary, on='Postcode', how='left')\n",
    "anon_data = anon_data.rename(columns={'Region': 'UK.Region'})\n",
    "\n",
    "# Assign hemisphere of birth depending on country of birth\n",
    "anon_data['Location.of.Birth'] = data['country_of_birth'].apply(lambda x: country_to_hemisphere(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "144a2b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add gaussian noise to weight/height, countries visited, and alcohol/smoking history\n",
    "weight_noise = np.random.normal(0,1,1000)*5\n",
    "anon_data['Weight'] = round(data['weight']+weight_noise, 1)\n",
    "\n",
    "height_noise = np.random.normal(0,1,1000)/5\n",
    "anon_data['Height'] = round(data['height']+height_noise, 2)\n",
    "bmi = data['weight'] / data['height']**2\n",
    "\n",
    "countries_noise = np.random.normal(0,1,1000)*5\n",
    "anon_data['Countries.Visited'] = round(data['n_countries_visited']+countries_noise)\n",
    "\n",
    "alcohol_noise = np.random.normal(0,1,1000)\n",
    "anon_data['Avg.Alcohol'] = round(data['avg_n_drinks_per_week']+alcohol_noise, 1)\n",
    "\n",
    "smoking_noise = np.random.normal(0,1,1000)*20\n",
    "anon_data['Avg.Cigarettes'] = round(data['avg_n_cigret_per_week']+smoking_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "e2aa7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach case-control status\n",
    "anon_data['CC.Status'] = data['cc_status']\n",
    "\n",
    "# Re-order columns\n",
    "anon_data = anon_data[['Sample.ID', 'Gender', 'Birthyear', 'Location.of.Birth', 'UK.Region', 'Weight', 'Height', 'Education.Level', 'Avg.Alcohol', 'Avg.Cigarettes', 'CC.Status']]\n",
    "\n",
    "# View the anonymised dataset\n",
    "anon_data\n",
    "\n",
    "# Output the files\n",
    "output_name = \"output\"\n",
    "anon_data.to_csv(output_name + \".csv\", sep=\",\", index=None)\n",
    "\n",
    "# Encrypt csv and delete original file\n",
    "encrypt(output_name + \".csv\", output_name + \"_encrypted.csv\", \"key.key\")\n",
    "os.remove(output_name + \".csv\")\n",
    "\n",
    "# Decrypt file\n",
    "decrypt(output_name + \"_encrypted.csv\", \"decrypted.csv\", \"key.key\")\n",
    "\n",
    "reference_table.to_csv(\"reference_table.csv\", sep=\",\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "f3a90bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Gender, Birthyear, Location.of.Birth, UK.Region, Education.Level, Count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Calculating K-anonymity\n",
    "\n",
    "# Checking k-anonymity for quasi-identifiers\n",
    "df_count = anon_data.groupby(['Gender', 'Birthyear', 'Location.of.Birth', 'UK.Region', 'Education.Level']).size().reset_index(name = 'Count') \n",
    "\n",
    "# Print those not meeting our specified k-anonymity level\n",
    "print(df_count[df_count['Count']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be85de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('hda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "caa27b422802b4fe8c2ce0d4ce59ab3cbffba11e0630735fb9c7a91a54a91b7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
